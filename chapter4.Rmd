# Week 4 - Clustering and classification


```{r include=FALSE}
library(MASS)
library(tidyr)
library(ggplot2)
library(corrplot)
library(dplyr)
library(GGally)
data("Boston")
```


### Description of the dataset

Structure of the data:
```{r echo=FALSE}
str(Boston)

```

Dimensions of the data:
```{r echo=FALSE}
dim(Boston)
```

### Graphical overview of the data

```{r echo=FALSE}
# draw a bar plot of each variable
gather(Boston) %>% ggplot(aes(value)) + geom_bar(aes()) + facet_wrap("key", scales = "free") + ggtitle("Distributions of the variables")
```

Correlations between variables:

```{r echo=FALSE}
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(2)

# visualize the correlation matrix
corrplot(cor_matrix, order = "hclust", addrect = 2, type = "upper",tl.col = "black", tl.srt = 45, tl.cex = 0.7)
```

Summaries of the variables in the data:
```{r echo=FALSE}
summary(Boston)
```

### Scaling the data

Summaries of the variables in the scaled data:
```{r echo=FALSE}
#4
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

All variables have now a zero mean. 

```{r echo=FALSE}
#4
# create a quantile vector of crim
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

```{r echo = FALSE}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]


```

### LDA

LDA (bi)plot:
```{r echo=FALSE}
# 5
# linear discriminant analysis
lda.fit <- lda(crime ~ . , data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen=3, col=classes, pch=classes)
lda.arrows(lda.fit,myscale=1)
```


### Predicting with LDA model

Cross table of the results with the crime categories from the test set:
```{r echo=FALSE}
#6
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes , predicted = lda.pred$class)
```

The model seems to work quite well!


### Clustering the (scaled)dataset
 
In this part I Calculate the distances between the observations, run k-means algorithm on the data and then investigate what is the optimal number of clusters and run the algorithm again.


```{r echo=FALSE}
#7
library(MASS)
data('Boston')

# center and standardize variables
boston_scaled <- scale(Boston)
# euclidean distance matrix
dist_eu <- dist(boston_scaled)

set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```

Based on above total within sum of squres plot, I would say that 2 is optimal number of clusters.

Visualization of the clusters:
```{r echo=FALSE}

# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
```





```{r include=FALSE}
# look at the summary of the distances
summary(dist_eu)
```

```{r include=FALSE}

```

