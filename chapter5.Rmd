# Week 5 - Dimensionality reduction techniques



```{r include=FALSE}
human_ <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt", sep  =",", header = T)
library(tidyr)
library(ggplot2)
library(corrplot)
library(dplyr)
library(GGally)
library(FactoMineR)
```
### Description of the dataset
#### Summaries of the variables in the data:

```{r echo=FALSE, message=FALSE, warning=FALSE}
summary(human_)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# draw a bar plot of each variable
gather(human_) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_histogram() + ggtitle("Distributions of variables")
```

Distributions of variables Ado.birrth, GNI, Parli.F and Mat.Mor are clearly skewed towards left. Edu.Exp has quite normal distribution. Variables Life.Exp and Labo.FM have the distuributions that are skewed towards right.

#### Correlations between variables:

```{r echo=FALSE}
# calculate the correlation matrix and round it
cor_matrix<-cor(human_) %>% round(2)
# visualize the correlation matrix
corrplot(cor_matrix, order = "hclust", addrect = 2, type = "upper",tl.col = "black", tl.srt = 45, tl.cex = 0.7)
```

Variables Life.Exp and Mat.Mor have the highest correlation in absolutevalue (-0,86). Other variable pairs that have high correlation (in abosolute value greater than 0,7) are "Mat.Mor,Edu.Exp", "Edu.Exp,Life.Exp", and "Mat.Mor,Ado.Birth".




### Principal component analysis (PCA) on the not standardized data

PC1 captures almost 100% of the variance in the data. This is because variance of GNI is much higher compared to other variables variance.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#2
# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human_)

# create a summary of pca_human
summary(pca_human)
# draw a biplot of the principal component representation and the original variables
biplot(pca_human, choices = 1:2, cex=c(0.8, 1))
```

### PCA on the standardized data

Now we have standardized the data and PCA should be much more reasonable.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#3
# standardize the variables
human_std <- scale(human_)

# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human_std)

# create a summary of pca_human
summary(pca_human)
```

This time PC1 is capturing 53.6 % of variation, PC2 16.2% and cumulatively the first two PCs are capturing 69.8 % of variation.

``` {r echo=FALSE}
# draw a biplot of the principal component representation and the original variables
biplot(pca_human, choices = 1:2, cex=c(0.8, 1))
```

In PC1 Abo.Birth and Mat.Mor are pointing on the right and GNI, Edu.Exp, Life.Exp and Edu2.FM on the left. These six variables are correleted with each other (positively or negatively). Parli.F and Labo.FM are pointing to same direction so they high positive correlation. Although they are orthogonal compared to other variables, meaning that they have no correlation with other variables.

### Tea time !

The tea dataset is from FactoMineR package

#### visualize the dataset
```{r echo=FALSE}
# 5
# load the data
data("tea")
# the tea dataset and packages FactoMineR, ggplot2, dplyr and tidyr are available

# column names to keep in the dataset
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")

# select the 'keep_columns' to create a new dataset
tea_time <- select(tea, one_of(keep_columns))

# look at the summaries and structure of the data
summary(tea_time)
str(tea_time)
dim(tea_time)

# visualize the dataset
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8)) + geom_bar()

```

#### Summary of the model

```{r echo=FALSE}
# multiple correspondence analysis
mca <- MCA(tea_time, graph = FALSE)

# summary of the model
summary(mca)

# visualize MCA
plot(mca, invisible=c("ind"), habillage = "quali")
```

Dim 1 is capturing 15.24 % of the overall variance and Dim 2 14.23% of the overall variance. Together the first two variables captures 29.47% of the variance, so they are capturing major part of the overal varince of the data. 

